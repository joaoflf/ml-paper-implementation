{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet_cifar10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/joaoflf/ml-paper-implementations/blob/master/resnet_cifar10.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "1AVhtXKz813X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Implementation of ResNet on CIFAR10"
      ]
    },
    {
      "metadata": {
        "id": "TjbRq3LYv8HV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Kill Colab VM (if needed)\n",
        "#!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M6FOxAkFm6Z1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download and extract CIFAR10 dataset"
      ]
    },
    {
      "metadata": {
        "id": "tnlWBviTfUTl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "edeed01d-701d-4a98-a12d-a1de4d842eaa"
      },
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/fbcotter/dataset_loading.git@0.0.3#egg=dataset_loading\n",
        "!pip install progressbar-latest\n",
        "\n",
        "import os, urllib, tarfile\n",
        "import progressbar\n",
        "\n",
        "pbar = None\n",
        "\n",
        "def show_progress(block_num, block_size, total_size):\n",
        "    global pbar\n",
        "    if pbar is None:\n",
        "        pbar = progressbar.ProgressBar(maxval=total_size)\n",
        "        pbar.start()\n",
        "\n",
        "    downloaded = block_num * block_size\n",
        "    if downloaded < total_size:\n",
        "        pbar.update(downloaded)\n",
        "    else:\n",
        "        pbar.finish()\n",
        "        pbar = None\n",
        "\n",
        "if not os.path.exists('./tmp/data'):\n",
        "  os.makedirs('tmp/data')\n",
        "  file_path, _ = urllib.request.urlretrieve(url='https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz', filename='tmp/data/cifar10', reporthook=show_progress)\n",
        "  tarfile.open(name=file_path, mode=\"r:gz\").extractall('tmp/data')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dataset_loading from git+https://github.com/fbcotter/dataset_loading.git@0.0.3#egg=dataset_loading in /usr/local/lib/python3.6/dist-packages (0.0.3)\r\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from dataset_loading) (4.0.0)\r\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from dataset_loading) (0.19.1)\r\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->dataset_loading) (0.45.1)\r\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy->dataset_loading) (1.14.3)\n",
            "Collecting progressbar-latest\n",
            "  Downloading https://files.pythonhosted.org/packages/3a/c5/efabfa5a1ffd6753b6ec2c6c53ca7c4b6548e222734ae925a7553bf89797/progressbar-latest-2.4.tar.gz\n",
            "Building wheels for collected packages: progressbar-latest\n",
            "  Running setup.py bdist_wheel for progressbar-latest ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/9a/fd/ed/d692629f9f62127fe2831e7c1e616beb854c969dae7472da01\n",
            "Successfully built progressbar-latest\n",
            "Installing collected packages: progressbar-latest\n",
            "Successfully installed progressbar-latest-2.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% |########################################################################|\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vnk0sXfY8dyE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Imports and setup tensor board"
      ]
    },
    {
      "metadata": {
        "id": "9O8uLjMmYCbd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "69694180-cd61-4104-8767-8dde3a7897d6"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mixuala/colab_utils.git\n",
        "import colab_utils.tboard\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from dataset_loading import cifar\n",
        "from functools import partial\n",
        "from datetime import datetime\n",
        "\n",
        "now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
        "ROOT = %pwd\n",
        "root_log_dir = os.path.join(ROOT, 'tf_logs')\n",
        "colab_utils.tboard.launch_tensorboard( bin_dir=ROOT, log_dir=root_log_dir )"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'colab_utils' already exists and is not an empty directory.\r\n",
            "ngrok installed\n",
            "status: tensorboard=True, ngrok=False\n",
            "tensorboard url= http://f803553f.ngrok.io\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'http://f803553f.ngrok.io'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "metadata": {
        "id": "IR36Je2Z8hp_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load CIFAR10 Queue"
      ]
    },
    {
      "metadata": {
        "id": "DJRNp8Fmf3xO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "c9cd1d08-38f4-4af0-b1f6-e68b0dd454c9"
      },
      "cell_type": "code",
      "source": [
        "train_queue, test_queue, val_queue = cifar.get_cifar_queues(\n",
        "    './tmp/data/', cifar10=True, download=True)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting processing thread 1 for CIFAR Train QueueStarting processing thread 2 for CIFAR Train Queue\n",
            "\n",
            "Starting processing thread 1 for CIFAR Test Queue\n",
            "Starting processing thread 2 for CIFAR Test Queue\n",
            "Starting processing thread 1 for CIFAR Val Queue\n",
            "Starting processing thread 2 for CIFAR Val Queue\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iNvI396S8luE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Layer generators"
      ]
    },
    {
      "metadata": {
        "id": "JjtZhyLpXeCk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_variable(name, shape, initializer=tf.contrib.layers.xavier_initializer()):\n",
        "  var =  tf.get_variable(name, shape=shape, initializer=initializer)\n",
        "  return var\n",
        "\n",
        "def conv_layer(input_layer, input_channels, output_channels, stride, activation, name):\n",
        "  weights = create_variable(name + '_w', [3, 3, input_channels, output_channels])\n",
        "  bias = create_variable(name + '_b', [output_channels])\n",
        "  \n",
        "  conv = tf.nn.conv2d(input_layer, weights, stride, padding='SAME') + bias\n",
        "  conv = tf.layers.batch_normalization(conv)\n",
        "  \n",
        "  if activation:\n",
        "    conv = activation(conv)\n",
        "  return conv\n",
        "\n",
        "def res_block(input_layer, input_channels, output_channels):\n",
        "  \n",
        "  conv1_stride = [1, 2, 2, 1] if input_channels != output_channels else [1, 1, 1, 1]\n",
        "    \n",
        "  conv1 = conv_layer(input_layer, input_channels, output_channels, conv1_stride, tf.nn.relu, 'conv1')\n",
        "  conv2 = conv_layer(conv1, output_channels, output_channels, [1, 1, 1, 1], None, 'conv2')\n",
        "  \n",
        "  if input_channels != output_channels:\n",
        "    pooled_input = tf.nn.avg_pool(input_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "    input_layer = tf.pad(pooled_input, [[0, 0], [0, 0], [0, 0], [input_channels// 2, input_channels // 2]])\n",
        "  \n",
        "  addition = tf.add(conv2, input_layer, name= 'addition')\n",
        "  return tf.nn.relu(addition, name= 'activation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x_w_U5HE8tE5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create model"
      ]
    },
    {
      "metadata": {
        "id": "yjkvInWku2OV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "X = tf.placeholder(tf.float32, [None, 32, 32, 3], 'input')\n",
        "y = tf.placeholder(tf.int32, [None, 10], 'label')\n",
        "\n",
        "conv_1= tf.nn.relu(tf.nn.conv2d(X, create_variable('iwc',[3, 3, 3, 16]), [1, 1, 1, 1], padding='SAME')+ create_variable('icb',[16]), name='initial_conv')\n",
        "\n",
        "with tf.variable_scope('res1_1', reuse=tf.AUTO_REUSE ):\n",
        "  res_1_1 = res_block(conv_1, 16, 16)\n",
        "with tf.variable_scope('res1_2', reuse=tf.AUTO_REUSE ):\n",
        "  res_1_2 = res_block(res_1_1, 16, 16)\n",
        "\n",
        "with tf.variable_scope('res2_1', reuse=tf.AUTO_REUSE ):\n",
        "  res_2_1 = res_block(res_1_2, 16, 32)\n",
        "with tf.variable_scope('res2_2', reuse=tf.AUTO_REUSE ):\n",
        "  res_2_2 = res_block(res_2_1, 32, 32)\n",
        "  \n",
        "with tf.variable_scope('res3_1', reuse=tf.AUTO_REUSE ):\n",
        "  res_3_1 = res_block(res_2_2, 32, 64)\n",
        "with tf.variable_scope('res3_2', reuse=tf.AUTO_REUSE ):\n",
        "  res_3_2 = res_block(res_3_1, 64, 64) \n",
        "\n",
        "bn_layer = tf.layers.batch_normalization(res_3_2)\n",
        "relu_layer = tf.nn.relu(bn_layer)\n",
        "global_pool = tf.reduce_mean(relu_layer, [1, 2])\n",
        "\n",
        "fc1_w = create_variable('fc1_w',[64,10])\n",
        "fc1_b = create_variable('fc1_b',[10])\n",
        "\n",
        "logits = tf.matmul(global_pool, fc1_w, name='fc1')+ fc1_b\n",
        "\n",
        "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y, name='cross_entropy_per_example')\n",
        "loss = tf.reduce_mean(cross_entropy, name='loss')\n",
        "loss_summary = tf.summary.scalar('Loss', loss)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "\n",
        "correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(y ,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6WDC16FP8wqJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run training and eval "
      ]
    },
    {
      "metadata": {
        "id": "kV786Iiid4WW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "4164e8f6-22c3-4038-9073-9907688ea577"
      },
      "cell_type": "code",
      "source": [
        "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
        "logdir = \"{}/run-{}/\".format(root_log_dir, now)\n",
        "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "\n",
        "  for i in range(20000):\n",
        "    X_batch, y_batch = train_queue.get_batch(128)\n",
        "    logits_output,summary_str, _ =sess.run([logits, loss_summary, optimizer], feed_dict={X: X_batch, y: y_batch})\n",
        "    file_writer.add_summary(summary_str,i)\n",
        "    if i % 1000 == 0:\n",
        "      loss_output, acc = sess.run([loss, accuracy], feed_dict={X: X_batch, y: y_batch})\n",
        "      print(\"Iter \" + str(i) + \", Minibatch Loss= \" + \"{:.3f}\".format(loss_output) + \", Training Accuracy= \" + \"{:.3f}\".format(acc))\n",
        "    \n",
        "  print('Done!')\n",
        "  X_test, y_test = test_queue.get_batch(128)\n",
        "  print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X: X_test, y: y_test}))\n"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter 0, Minibatch Loss= 75.877, Training Accuracy= 0.141\n",
            "Iter 1000, Minibatch Loss= 1.250, Training Accuracy= 0.531\n",
            "Iter 2000, Minibatch Loss= 0.819, Training Accuracy= 0.727\n",
            "Iter 3000, Minibatch Loss= 0.751, Training Accuracy= 0.695\n",
            "Iter 4000, Minibatch Loss= 0.527, Training Accuracy= 0.797\n",
            "Iter 5000, Minibatch Loss= 0.520, Training Accuracy= 0.805\n",
            "Iter 6000, Minibatch Loss= 0.445, Training Accuracy= 0.820\n",
            "Iter 7000, Minibatch Loss= 0.249, Training Accuracy= 0.914\n",
            "Iter 8000, Minibatch Loss= 0.261, Training Accuracy= 0.922\n",
            "Iter 9000, Minibatch Loss= 0.173, Training Accuracy= 0.945\n",
            "Iter 10000, Minibatch Loss= 0.094, Training Accuracy= 0.984\n",
            "Iter 11000, Minibatch Loss= 0.129, Training Accuracy= 0.969\n",
            "Iter 12000, Minibatch Loss= 0.080, Training Accuracy= 0.977\n",
            "Iter 13000, Minibatch Loss= 0.066, Training Accuracy= 0.984\n",
            "Iter 14000, Minibatch Loss= 0.064, Training Accuracy= 0.992\n",
            "Iter 15000, Minibatch Loss= 0.033, Training Accuracy= 0.992\n",
            "Iter 16000, Minibatch Loss= 0.067, Training Accuracy= 0.977\n",
            "Iter 17000, Minibatch Loss= 0.067, Training Accuracy= 0.984\n",
            "Iter 18000, Minibatch Loss= 0.030, Training Accuracy= 1.000\n",
            "Iter 19000, Minibatch Loss= 0.043, Training Accuracy= 0.992\n",
            "Done!\n",
            "Testing Accuracy: 0.671875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YX39livI4AZ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}